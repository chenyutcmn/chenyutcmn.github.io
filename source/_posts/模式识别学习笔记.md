---
title: 国科大模式识别学习笔记
date: 2019-01-02 20:31:53
categories: note
tags: 模式识别
---

国科大模式识别课程学习笔记
<!--more-->

--- 

## 统计判别
#### 贝叶斯判别

![贝叶斯判别公式](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01rcx5j308v02yq2q.jpg) 

需要的条件为两类模式的概率，X在两类模式下的分布。 

当考虑到对于某一类的错误判决要比对另一类的判决更为关键时，就需要把最小错误概率的贝叶斯判别做一些修正，提出条件平均风险。在以上公式基础上进行加权求和。 

在多类情况下，最小平均风险的判别式为 

![贝叶斯最小平均风险判别式](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01qqxdj307d016jr5.jpg) 

---

#### 例：M种多变量正态类密度模式的分类
具有M种模式类别的多变量正态类密度函数为： 

![正态分布下的概率密度函数](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01qtsgj30ee01t743.jpg) 


已知类别ωi的判别函数可写成如下形式： 

![贝叶斯判别公式](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01qqxdj307d016jr5.jpg)
 


假设有两类模式，则判别界面为d1-d2.(多类情况下对应情况3，可得出该类的判别界面)。

---

#### 均值向量和协方差矩阵的参数估计

现有一正态分布，设θ是其均值，即带估计参数，目标是迭代求θ的概率密度函数。即已知 

![参数估计中的概率密度函数形式](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01qg43j307f02udfm.jpg) 


并给出N个训练样本{x1, x2,…, xN}，用贝叶斯学习计算其均值估计量。具体迭代公式略。
当使用贝叶斯判别时，只是用样本来计算各模式的概率有失偏颇。这时，若有一个先验估计θ作为样本均值的估计，则可以通过加权和的方式与样本均值一起得出样本整体的概率分布。

---
---

## 判别函数
上一章讲，通过统计来获得判别界面。这种方式需要知道两类模式各自出现的概率，和两类样本的概率密度函数。这一章，讲直接通过样本来计算判别界面，依赖变少。
#### 线性判别函数
两类情况略，多类情况分为以下三种 

1. 即把M类多类问题分成M个两类问题，因此共有M个判别函数。
2. 采用每对划分，即ωi/ωj两分法，此时一个判别界面只能分开两种类别，但不能把它与其余所有的界面分开。
3. 这是没有不确定区域的ωi/ωj两分法。该分类的特点是把M类情况分成M-1个两类问题。（对应贝叶斯判别界面的确定）

---

#### 非线性判别函数
分两种方式
1. 特征变换，是广义的线性判别，略。
2. 分段线性判别逼近非线性判别。 

---

#### Fisher线性判别

将n维特征向量投影至一维，目标函数为： 

![Loss function](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01rb7fj303301m0sh.jpg)


分子分母分别与类间散度矩阵，样本总类内散度矩阵有关。 

对J求导,令分母等于非零常数，即 

![推倒过程](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01sbawj302u00x0je.jpg)


定义Lagrange函数为：

![拉格朗日函数](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01sipkj30690113y9.jpg)


将上式对w求偏导数，经过一系列推倒可得：

![最优解](http://ww1.sinaimg.cn/large/007xDx5ily1fyud01ti8mj303l0150oc.jpg)


这种算法将所有样本输入来得到分类器，计算复杂度高。

---

#### 感知器算法

感知器算法就是通过训练样本模式的 **迭代** 和学习，产生线性（或广义线性）可分的模式判别函数。**不需要对各类别中模式的统计性质做任何假设**，因此称为确定性的方法。

---

#### 可训练的确定性分类器的迭代算法

这里我们将引入更广泛意义上的迭代算法。 

在感知器算法中，判别界面的迭代根据样本来选取，但这样的迭代并不是速度最快的，这时，我们引入准则函数J(w,x)，根据他的梯度来迭代权向量w。 

设取准则函数为：

![准则函数](https://ws1.sinaimg.cn/large/007xDx5ily1fyvh56iquvj305001w741.jpg)

那么梯度法将退化成感知算法 

感知器算法只是当被分模式可用一个特定的判别界面分开时才收敛，在不可分情况下，只要计算程序不终止，它就始终不收敛。
即使在模式可分的情况下，也很难事先算出达到收敛时所需要的迭代次数。 

最小平方误差(LMSE)算法，除了对可分模式是收敛的以外，对于类别不可分的情况也能指出来。定义准则函数：

![LMSE准则函数](https://ws1.sinaimg.cn/large/007xDx5ily1fyvh7w2z6uj30cf01gdfm.jpg)


根据迭代算法可判断是否可分，考试不考，具体细节略。 

---

#### 势函数法 一种确定性的非线性分类方法

在训练状态，模式样本逐个输入分类器，分类器就连续计算相应的势函数，在第k步迭代时的积累位势决定于在该步前所有的单独势函数的累加。 

一般来说，若两个n维向量x和xk的函数K(x, xk)同时满足下列三个条件，则可作为势函数。
-	K(x, xk)=K(xk, x)，并且当且仅当x=xk时达到最大值；
-	当向量x与xk的距离趋于无穷时，K(x, xk)趋于零；
-	K(x, xk)是光滑函数，且是x与xk之间距离的单调下降函数。

第一类势函数：可用对称的有限多项式展开，即：

![势函数法升维公式](https://ws1.sinaimg.cn/mw690/007xDx5ily1fyvhdxsnxaj304s01d741.jpg)


这种方法可以看成是升维的一种方式。在作业题中，因为原问题线性不可分，现有两种方式升维，一种将2维映射至4维，选取H0和H2，一种将2维映射至9维（未校验，可能实际不需要这么多），选取H0，H1，H2。第一种方式，将w1中的两个点映射为了一个点，w2同理，以实现线性可分，私以为不可取。  

第二类势函数：选择双变量x和xk的对称函数作为势函数，即K(x, xk) = K(xk, x)，并且它可展开成无穷级数，例如：


![第二类势函数法](https://ws1.sinaimg.cn/mw690/007xDx5ily1fyvhjeye3lj303p01d0qf.jpg)

 
**可能**为一种曲线划分方式，理解存疑。

---

#### 决策树
利用树分类器可以把一个复杂的多类别分类问题，转化为若干个简单的分类问题来解决，采用分级的形式，使分类问题逐步得到解决。 

二叉树结构分类器概念简单、直观、便于解释，而且在各个节点上可以选择不同的特征和采用不同的决策规则，因此设计方法灵活多样，便于利用先验知识来获得一个较好的分类器。
在设计一个决策树时，主要应解决以下几个问题：
-	选择一个合适的树结构，即合理安排树的节点和分支；
-	确定在每个非终止节点上要使用的特征；
-	在每个非终止节点上选择合适的决策规则

---
---

## 特征选择和特征提取 
#### 特征选择

假若类概率密度函数不是或不近似正态分布，均值和方差就不足以用来估计类别的可分性，此时该准则函数不完全适用。
离散K-L变换

#### 离散K-L变换

将原来的特征做正交变换，获得的每个数据都是原来n个数据的线性组合，然后从新的数据中选出少数几个，使其尽可能多地反映各类模式之间的差异，而这些特征间又尽可能相互独立。

K-L展开式用于特征选择相当于一种线性变换。
若从n个特征向量中取出m个组成变换矩阵Φ，即

![特征选择](https://ws1.sinaimg.cn/mw690/007xDx5ily1fyvhpopk2ij30430150nt.jpg)

此时，Φ是一个n*m维矩阵，x是n维向量，经过ΦTx变换，即得到降维为m的新向量。

若采用较大特征值对应的特征向量组成变换矩阵，则能对应地保留原模式中方差最大的特征成分，所以K-L变换起到了减小相关性、突出差异性的效果。在此情况下， K-L变换也称为主成分变换（PCA变换）。

---
---

## 小结
以上内容具有内在联系，试想，在多个模式空间所有样本均已知，即已知该模式下的样本概率密度函数，每个模式的概率也已知，这时，用贝叶斯分类即可判别任意样本的类别。 

但这是不现实的，我们只能得到样本集的采样数据，这时我们可以通过求已知样本的概率密度函数，来拟合整体样本集的概率密度，并估计各模式的先验概率，来进行贝叶斯判别。

这时，通过对P(θ|X)的计算，可以得到Di和判别界面即f(x)(三种模式划分方式)。以fisher判别为例，可以将判别函数看成是将n维特征映射到了1维(y)来进行简单的划分。但fisher判别的缺点是需要一次性计算所有样本，这时，便引出了感知器算法来进行迭代求解。

再对感知器算法进一步深入，便可得到可训练的确定性分类器的迭代算法。

---
---

## 统计机器学习基础
Statistical modeling is a formalization of relationships between variables in the data in the form of mathematical equations. 

统计建模是以数学方程的形式对数据中变量之间的关系进行形式化的。 

也就是说，通过数学语言描述样本集的内在联系，去近似实际样本的整体分布。 

本章主要描述一种在训练集和测试集间的平衡。即模型在训练集上的拟合程度，和在实际中的泛化性的平衡。若模型在训练集上拟合程度高，代表模型的偏差小，参数复杂，但此时实际上的拟合程度差，代表模型的方差大，出现过拟合。 


![特征选择](https://ws1.sinaimg.cn/mw690/007xDx5ily1fyvi259q98j30lv04374k.jpg)

上面的式子中，第一行代表模型预测值与真实值的偏离程度，第二行的第一项代表着预测值与样本集上能达到的最优值的偏离，第二项代表样本最优与实际真实值的偏离。其中，第二项是由于样本集的选取造成的，是因为样本集抽样方式不当，或抽样时伴随误差。 

看回第一项


![](https://ws1.sinaimg.cn/mw690/007xDx5ily1fyvi7fg308j30it03u0t1.jpg)


可以看到，第一项又可以分为多次模型的均值与最优值之间的偏离和模型本身的方差。将其称之为偏差与方差。

![](https://ws1.sinaimg.cn/large/007xDx5ily1fyvibgjbw1j30ka0av75c.jpg)

如图，图一的偏差最大，方差最小，代表模型的复杂度低；图三的偏差小，方差大，代表模型复杂度高。要像图二一样取得一个平衡。

那么，我们引出正则项来控制模型的复杂程度。公式略。

常用的正则有L1正则和L2正则，如图

![](https://ws1.sinaimg.cn/large/007xDx5ily1fyvifbd2pej30gl0a3mxl.jpg)


L2正则取欧式距离最小，而L1正则则会取部分权重为0。特点在于L1正则起到了一个特征选择的作用。

--- 
---

## 有监督学习方法

**本章讲解方法的概率上的解释**

#### Probabilistic Explanation: 极大化似然函数 MLE

以线性回归为例，现有样本集N，求一线性函数拟合数据点，即求参数向量W。定义准则函数：
![](https://ws1.sinaimg.cn/large/007xDx5ily1fyw3ozqkajj309902k3yf.jpg)
对J求梯度，迭代计算即可。 

这里，我们来解释为什么选取该准则函数。在**估计的W**下，有似然函数
![](https://ws1.sinaimg.cn/large/007xDx5ily1fyw3sg93g6j30el02ht8o.jpg)
我们的预测值WT*X与真实值之间是有差距的，这里我们假设真实值y与服从以预测值为均值的正态分布，那么，在参数为W的情况下，由样本X得到其真实值y的概率为 
![](https://ws1.sinaimg.cn/large/007xDx5ily1fyw3yhlpumj30do02omx4.jpg)
对L(W)取对数，有
![](https://ws1.sinaimg.cn/large/007xDx5ily1fyw40ugho4j30hr071wey.jpg)

可以看出，极大化似然函数等价于
![](https://ws1.sinaimg.cn/large/007xDx5ily1fyw42uepxvj306p02uwed.jpg)


一些概率上的解释见[似然和似然函数详解](https://zhuanlan.zhihu.com/p/42598338)

---

#### 极大化后验概率 MAP
思路同上，具体见下图
![](https://ws1.sinaimg.cn/large/007xDx5ily1fyw4cyd270j30ve0eyju4.jpg)


两者区别见下图
![](https://ws1.sinaimg.cn/large/007xDx5ily1fyw4m4cp57j30uh0fkdj3.jpg)

极大化后验概率相当于极大化似然函数加先验

---

#### 生成模型 vs 判别模型

- 生成模型是对P(x|y)建模然后通过贝叶斯公式去估计P(y|x),判别式模型是直接对后验概率P(y|x)建模。

- 生成式模型因为要对模式的概率密度函数建模，所以模型要更加复杂。而判别式模型相对简单。

- 生成式模型对模式的概率是有先验估计的，所以用可以用更少的的数据达到更好的效果。

- 对模式的先验估计出错的情况下，生成式模型将得到相对更加错误的结果。

#### 两类模型的一些例子

以后写(hhhhh)

##SVM

是对感知机模型的改进，得到最大化间隔分类超平面。这一段的证明与KKT条件有关，了解最优化原理对理解下面的推倒很有帮助

现在考虑已经得到判别函数，则判别规则为

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2wdo5ddlj30bh02j747.jpg)

也即 

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2wesv584j306b01xglg.jpg)

那么我们现在的优化目标是最大化最小间隔。现假设得到最小间隔的样本x经过计算得到输出γ，即目标是最大化γ。 

至于为什么使用几何间隔，是因为γ的值与w向量的长度有关，在训练过程中他的值会随w的变换而变化，必须做归一化处理，此时得到下面的优化问题

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2xhhsppgj30i305iq34.jpg)

这一步，我们同时将w引入了目标函数，继续转换得（下面为最小化w的二模）

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2xum9yguj30hp0520sv.jpg)

引入拉格朗日乘子

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2z5s4e9aj30k707d0tb.jpg)

求导

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2z75cesqj309t04p3yl.jpg)

将其代回上式，得 

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2zee1308j30gv04nwel.jpg)

那么原问题就转化为了对偶问题

![](https://ws1.sinaimg.cn/large/007xDx5ily1fz2zfpp7e4j30g104ljrn.jpg)

此时问题规模仅与样本集大小有关(对应于原问题中与样本纬度有关)，利于核方法的应用。



